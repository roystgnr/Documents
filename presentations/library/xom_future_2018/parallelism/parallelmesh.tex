\subsection{ParallelMesh}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\royslide{SerialMesh Partitioning}{
\begin{columns}
\begin{column}{.5\textwidth}
  \royitemizebegin{}
    \item Each element, node is ``local'' to one processor
    \item Each processor has an identical Mesh copy
    \item Mesh stays in sync through redundant work
    \item FEM data synced on ``ghost'' elements only
  \royitemizeend
\end{column}
\begin{column}{.5\textwidth}
  \begin{center}
    \includegraphics[width=.9\textwidth]{parallelism/SerialMesh}
  \end{center}
\end{column}
\end{columns}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\royslide{ParallelMesh Partitioning}{
\begin{columns}
\begin{column}{.5\textwidth}
  \royitemizebegin{}
    \item Processors store only local and ghost objects
    \item Each processor has a small Mesh subset
    \item Mesh stays in sync through MPI communication
  \royitemizeend
\end{column}
\begin{column}{.5\textwidth}
  \begin{center}
    \includegraphics[width=.9\textwidth]{parallelism/ParallelMesh1}
  \end{center}
\end{column}
\end{columns}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\royslide{ParallelMesh Partitioning}{
\begin{columns}
\begin{column}{.5\textwidth}
  \royitemizebegin{Pros}
    \item Reduced memory use
    \item $O(N_E/N_P)$ CPU costs
  \royitemizeend
\end{column}
\begin{column}{.5\textwidth}
  \begin{center}
    \includegraphics[width=.9\textwidth]{parallelism/ParallelMesh2}
  \end{center}
\end{column}
\end{columns}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\royslide{ParallelMesh Partitioning}{
\begin{columns}
\begin{column}{.5\textwidth}
  \royitemizebegin{Cons}
    \item Increased code complexity
    \item Increased synchronization ``bookkeeping''
    \item Greater debugging difficulty
  \royitemizeend
\end{column}
\begin{column}{.5\textwidth}
  \begin{center}
    \includegraphics[width=.9\textwidth]{parallelism/ParallelMesh3}
  \end{center}
\end{column}
\end{columns}
}


%===============================================================================
% NEW SLIDE
%===============================================================================
\begin{frame}
\frametitle{``Typical'' PDE example}

Transient Cahn-Hilliard, Bogner-Fox-Schmidt quads or hexes

  \includegraphics[width=.5\textwidth]{parallelism/parallelmesh_usage_2D}
  \includegraphics[width=.5\textwidth]{parallelism/parallelmesh_usage_3D}

\begin{block}{Results}
\begin{itemize}
\item Parallel codes using SerialMesh are unchanged for ParallelMesh
\item Overhead, distributed sparse matrix costs are unchanged
\item Serialized mesh, indexing once dominated RAM use
\end{itemize}
\end{block}
\end{frame}


%===============================================================================
% NEW SLIDE
%===============================================================================
\begin{frame}
\frametitle{Distributed Mesh Refinement}

\begin{columns}
\column{.6\textwidth}
\begin{block}{Elem, Node creation}
\begin{itemize}
\item<2-> Ids $\{i: i\;{\mathrm{mod}}\;(N_P+1) = p\}$ are owned by processor $p$
\end{itemize}
\end{block}

\visible<3->{
\begin{block}{Synchronization}
\begin{itemize}
\item Refinement Flags
\begin{itemize}
  \item<4-> Data requested by id
  \item<4-> Iteratively to enforce smoothing
\end{itemize}
\item New ghost child elements, nodes
\begin{itemize}
\item<4-> Id requested by data
\end{itemize}
\item Hanging node constraint equations
\begin{itemize}
\item<4-> Iteratively through subconstraints,
subconstraints-of-subconstraints...
\end{itemize}
\end{itemize}
\end{block}
}
\column{.4\textwidth}
\begin{center}
\includegraphics[width=.6\textwidth]{parallelism/LevelOneProblem}
\end{center}
\end{columns}

\end{frame}

%\section{Applications}

